{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26b6bcaa-fadb-47df-92e2-b8685f3991dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d4c91fa-9daf-45d1-870e-b79c761cc786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-9.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 35.3 MB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from pyarrow) (1.22.4)\n",
      "Installing collected packages: pyarrow\n",
      "\u001b[33m  WARNING: The script plasma_store is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed pyarrow-9.0.0\n",
      "Collecting fastparquet\n",
      "  Downloading fastparquet-0.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from fastparquet) (21.3)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from fastparquet) (1.4.3)\n",
      "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.8/dist-packages (from fastparquet) (1.22.4)\n",
      "Collecting cramjam>=2.3.0\n",
      "  Downloading cramjam-2.5.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 110.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from fastparquet) (2022.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->fastparquet) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.0->fastparquet) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.0->fastparquet) (1.16.0)\n",
      "Installing collected packages: cramjam, fastparquet\n",
      "Successfully installed cramjam-2.5.0 fastparquet-0.8.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow\n",
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc192bf-0311-4648-8e96-f7a467dc2136",
   "metadata": {},
   "source": [
    "# Note : We keep perennial crops aside"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09d6ccd-33ee-45d9-8ee9-86351b12a099",
   "metadata": {},
   "source": [
    "# Custom modeling : cv or nested cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf368da9-1b9f-44b1-9146-2fa556e1edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, balanced_accuracy_score, f1_score\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    KFold,\n",
    "    ShuffleSplit,\n",
    "    StratifiedKFold,\n",
    "    StratifiedShuffleSplit,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07d44483-1387-4c51-a65f-de8f138fee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncpus = 4\n"
     ]
    }
   ],
   "source": [
    "ncpus = round(get_cpu_quota())\n",
    "print(\"ncpus = \" + str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448dc9ee-bcaa-4641-9b48-6995d323734b",
   "metadata": {},
   "source": [
    "##### base experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a24891b1-d9d3-4cc0-befb-5ff6e4d2b994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    22\n",
      "0.0    19\n",
      "2.0     8\n",
      "5.0     7\n",
      "8.0     5\n",
      "4.0     4\n",
      "6.0     4\n",
      "Name: field, dtype: int64\n",
      "(69, 37)\n"
     ]
    }
   ],
   "source": [
    "# 0.2, 0.6\n",
    "model_input = pd.read_parquet('./data/gm_1.parquet')\n",
    "\n",
    "model_input_sampled = model_input[model_input['field'].isin([1,0,2,5])].groupby('field').sample(frac=0.2)\n",
    "model_input_non_sampled = model_input[model_input['field'].isin([8, 6, 4])].groupby('field').sample(frac=0.6)\n",
    "print(pd.concat([model_input_sampled, model_input_non_sampled], ignore_index=True)['field'].value_counts())\n",
    "print(pd.concat([model_input_sampled, model_input_non_sampled], ignore_index=True).shape)\n",
    "\n",
    "model_input_sampled = pd.concat([model_input_sampled, model_input_non_sampled], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfa98c46-c25a-4e03-aa84-3ec27c20f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_input_sampled.drop('field', axis=1).values\n",
    "y = model_input_sampled[['field']].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a15fb16a-6a1e-4c11-9ba0-6b5e990e04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store models\n",
    "models = []\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"RandomForest\"\n",
    "\n",
    "rf_param_grid = {\n",
    "    \"model__class_weight\": [\"balanced\", None],\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\", None],\n",
    "    \"model__n_estimators\": [200, 300, 400],\n",
    "    \"model__criterion\": [\"gini\", \"entropy\"],\n",
    "}\n",
    "\n",
    "models.append((model_name, RandomForestClassifier(n_jobs=1), rf_param_grid))\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"AdaBoostClassifier\"\n",
    "\n",
    "ab_param_grid = {\n",
    "    \"model__base_estimator\": [DecisionTreeClassifier(max_depth=i) for i in [1, 3, 10]],\n",
    "    \"model__n_estimators\": [10, 100, 1000],\n",
    "    \n",
    "    \"model__learning_rate\": [0.01, 0.1, 1],\n",
    "}\n",
    "\n",
    "models.append((model_name, AdaBoostClassifier(), ab_param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e22cce1-afbb-4b30-bff8-d4f66575afe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RandomForest\n",
      "running Outer Split 0\n",
      "    fitting inner CV loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing prediction\n",
      "running Outer Split 1\n",
      "    fitting inner CV loop\n",
      "performing prediction\n",
      "running Outer Split 2\n",
      "    fitting inner CV loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing prediction\n",
      "Running AdaBoostClassifier\n",
      "running Outer Split 0\n",
      "    fitting inner CV loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing prediction\n",
      "running Outer Split 1\n",
      "    fitting inner CV loop\n",
      "performing prediction\n",
      "running Outer Split 2\n",
      "    fitting inner CV loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing prediction\n"
     ]
    }
   ],
   "source": [
    "# Create empty lists to store outputs\n",
    "results = {}\n",
    "outer_cv_test_pairs = {}\n",
    "pipelines = {}\n",
    "\n",
    "# Only run a single trial for each algorithm, so set a single seed to use for selecting folds\n",
    "cv_seed = 13\n",
    "model_seed = 32\n",
    "\n",
    "# Set number of splits to do\n",
    "inner_cv_splits = 3\n",
    "outer_cv_splits = 3\n",
    "\n",
    "# Number of jobs to pass to the inner cross validation loop\n",
    "n_jobs_outer = 3\n",
    "n_jobs_inner = ncpus - n_jobs_outer\n",
    "\n",
    "for name, model, p_grid in models:\n",
    "    print(f\"Running {name}\")\n",
    "\n",
    "    # Create the pipeline method to leverage\n",
    "    pipeline = Pipeline(\n",
    "            steps=[\n",
    "                (\"model\", model),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    pipelines[name] = pipeline\n",
    "\n",
    "    # Create the outer_cv for each model so that the same data is fitted\n",
    "    outer_cv = StratifiedKFold(\n",
    "        n_splits=outer_cv_splits, shuffle=True, random_state=cv_seed\n",
    "    )\n",
    "\n",
    "    # Create dictionary to store testing arrays for each model\n",
    "    model_cv_test_pairs = {}\n",
    "    model_best_estimators = {}\n",
    "\n",
    "    # Loop over the outer split\n",
    "    for outer_split_number, (train_index, test_index) in enumerate(\n",
    "        outer_cv.split(X, y)\n",
    "    ):\n",
    "        print(f\"running Outer Split {outer_split_number}\")\n",
    "\n",
    "        X_train, X_test = X[train_index, :], X[test_index, :]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Create inner cv for each outer cv\n",
    "        inner_cv = StratifiedKFold(\n",
    "            n_splits=inner_cv_splits, shuffle=True, random_state=cv_seed\n",
    "        )\n",
    "\n",
    "        # Create grid search\n",
    "        clf = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grid=p_grid,\n",
    "            scoring=\"f1_macro\",\n",
    "            cv=inner_cv,\n",
    "            n_jobs=n_jobs_inner,\n",
    "        )\n",
    "\n",
    "        print(\"    fitting inner CV loop\")\n",
    "        # Fit to training data\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Calculate prediction\n",
    "        best_model = clf.best_estimator_\n",
    "        print(\"performing prediction\")\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Calculate metrics\n",
    "        test_f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "        # Store the results\n",
    "        model_best_estimators[f\"split_{outer_split_number}\"] = {\n",
    "            \"best_estimator\": clf.best_estimator_,\n",
    "            \"f1_macro_score\": test_f1_macro,\n",
    "        }\n",
    "\n",
    "        # Store the true and predicted arrays\n",
    "        model_cv_test_pairs[f\"split_{outer_split_number}\"] = (y_test, y_pred)\n",
    "\n",
    "    # Capture results out\n",
    "    outer_cv_test_pairs[name] = model_cv_test_pairs\n",
    "    results[name] = model_best_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24b83a-f353-42fa-bbe2-b310d1febcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best estimated params for RF model\n",
    "prefered_model = models[0]\n",
    "outer_cv = StratifiedKFold(n_splits=outer_cv_splits, shuffle=True, random_state=cv_seed)\n",
    "\n",
    "metric = \"f1_macro\"\n",
    "name, model, p_grid = prefered_model\n",
    "\n",
    "# instatiate a gridsearchCV using outer cross-validation folds\n",
    "clf = GridSearchCV(\n",
    "    pipelines[name],\n",
    "    p_grid,\n",
    "    scoring=metric,\n",
    "    verbose=1,\n",
    "    cv=outer_cv.split(X, y),\n",
    "    n_jobs=ncpus,\n",
    ")\n",
    "\n",
    "# Fit the gridsearch on outer cross-validation folds\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(\"The most accurate combination of tested parameters is: \")\n",
    "pprint(clf.best_params_)\n",
    "print(\"\\n\")\n",
    "print(\"The \" + metric + \" score using these parameters is: \")\n",
    "print(round(clf.best_score_, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a03b58-77f2-479a-b463-ab6ac5cb9c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if remove_correlated_features:\n",
    "    removed_cols = clf.best_estimator_[\"drop_corr_features\"].to_drop\n",
    "    remaining_cols = [col for col in columns_to_use if col not in removed_cols]\n",
    "else:\n",
    "    remaining_cols = columns_to_use\n",
    "\n",
    "remaining_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f980a395-d671-4f7e-acba-6bf6d1ce9b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data and fit new model\n",
    "X_transformed = clf.best_estimator_[\"drop_corr_features\"].transform(X)\n",
    "\n",
    "new_model = clf.best_estimator_[\"model\"]\n",
    "new_model.fit(X_transformed, y)\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "if not os.path.exists(\"results\"):\n",
    "    os.makedirs(\"results\")\n",
    "\n",
    "# Export the final model for use in following notebooks\n",
    "dump(new_model, f\"results/{experiment_name}_{name}.joblib\")\n",
    "\n",
    "# Export the columns to use in the final model\n",
    "with open(\n",
    "    f\"results/{experiment_name}_{name}_features.json\", \"w\", encoding=\"utf-8\"\n",
    ") as f:\n",
    "    json.dump({\"features\": remaining_cols}, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
