{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "737939b7-d72a-4a8c-a995-bdc52f194804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/geopandas/_compat.py:112: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "def create_geometries_from_manual_edit(df, geom_col):\n",
    "\n",
    "    # Raise a KeyError if geom_col does not appear in the dataframe\n",
    "    if geom_col not in df.columns:\n",
    "        raise KeyError(f\"{geom_col} is not a valid column of this dataframe.\")\n",
    "\n",
    "    # Convert from string `lat,lon` to individual columns for lat and lon\n",
    "    df[[\"point_lat\", \"point_lon\"]] = df[geom_col].str.split(\",\", expand=True, n=2)\n",
    "    \n",
    "    # Add a point location column, specify as manual to differ from ODK original values\n",
    "    df.loc[:, \"point_location\"] = \"manual\"\n",
    "\n",
    "    # Convert to geopandas with point geometry\n",
    "    geom = gpd.points_from_xy(x=df.point_lon, y=df.point_lat, crs=\"EPSG:4326\")\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geom)\n",
    "\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def create_geometries_from_ODK(df):\n",
    "\n",
    "    # identify rows with either center measurement or corner measurement\n",
    "    corner_measurement = df.loc[:, \"access_consent\"] == \"no\"\n",
    "    center_measurement = df.loc[:, \"access_consent\"] == \"yes\"\n",
    "\n",
    "    # create a new column to specify location type\n",
    "    df.loc[corner_measurement, \"point_location\"] = \"outside_corner\"\n",
    "    df.loc[center_measurement, \"point_location\"] = \"center\"\n",
    "\n",
    "    # create a column to store the lat,lon values\n",
    "    df.loc[corner_measurement, \"point_latlon\"] = df.loc[\n",
    "        corner_measurement, \"field_outside_corner\"\n",
    "    ]\n",
    "    df.loc[center_measurement, \"point_latlon\"] = df.loc[\n",
    "        center_measurement, \"field_center\"\n",
    "    ]\n",
    "\n",
    "    # Convert from string `lat,lon` to individual columns for lat and lon\n",
    "    point_latlon_df = pd.DataFrame(\n",
    "        df[\"point_latlon\"].str.split(\",\").to_list(), columns=[\"point_lat\", \"point_lon\"]\n",
    "    )\n",
    "    df = pd.concat((df, point_latlon_df), axis=\"columns\")\n",
    "\n",
    "    # Add boundaries for any locations with center points\n",
    "    df.loc[center_measurement, \"field_boundary\"] = df.loc[\n",
    "        center_measurement, \"field_boundary\"\n",
    "    ].str.split(\"; \")\n",
    "\n",
    "    # Separate out items where boundary exists to perform next steps\n",
    "    df_withboundary = df.loc[center_measurement, (\"KEY\", \"field_boundary\")]\n",
    "\n",
    "    # Explode all boundaries to get one row per point in boundary, then split into lat, lon, altitude and accuracy\n",
    "    df_withboundary = df_withboundary.explode(\"field_boundary\")\n",
    "    df_withboundary[[\"lat\", \"lon\", \"alt\", \"acc\"]] = df_withboundary[\n",
    "        \"field_boundary\"\n",
    "    ].str.split(\" \", expand=True, n=4)\n",
    "    df_withboundary = df_withboundary.drop(\n",
    "        [\"field_boundary\", \"alt\", \"acc\"], axis=\"columns\"\n",
    "    )\n",
    "\n",
    "    # Convert into Polygons using groupby\n",
    "    df_withboundary[\"field_boundary_polygon\"] = df_withboundary.groupby(\n",
    "        df_withboundary.index\n",
    "    ).apply(lambda g: Polygon(gpd.points_from_xy(g[\"lon\"], g[\"lat\"])))\n",
    "\n",
    "    # Drop duplicates\n",
    "    df_withboundary = df_withboundary.drop([\"lat\", \"lon\"], axis=\"columns\")\n",
    "    df_withboundary = df_withboundary.drop_duplicates(subset=\"KEY\")\n",
    "\n",
    "    df = df.set_index(\"KEY\").join(df_withboundary.set_index(\"KEY\"), how=\"left\")\n",
    "\n",
    "    # Convert to geopandas with point geometry\n",
    "    geom = gpd.points_from_xy(x=df.point_lon, y=df.point_lat, crs=\"EPSG:4326\")\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geom)\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25798696-abe6-4db5-814b-234245b1bc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/usr/local/lib/python3.8/dist-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/usr/local/lib/python3.8/dist-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from datacube.utils import geometry\n",
    "from deafrica_tools.classification import collect_training_data\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "330f11b2-0b4a-4ef8-beae-52de8d5ecd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_file = \"https://raw.githubusercontent.com/digitalearthafrica/crop-type/main/1_Prepare_samples_for_ML/data/datasheet.csv\"\n",
    "samples_df = pd.read_csv(samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e01b10-cca6-4973-bf11-932e5fe0d13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove prefixes from columns to improve readability\n",
    "updated_columns = samples_df.columns.str.replace(\"data-\", \"\")\n",
    "updated_columns = updated_columns.str.replace(\"consent_given-\", \"\")\n",
    "updated_columns = updated_columns.str.replace(\"field_planted-\", \"\")\n",
    "samples_df.columns = updated_columns\n",
    "# Convert from missing values being \" \" to None\n",
    "samples_df = samples_df.replace({\" \": None})\n",
    "# Convert date columns to date strings. Must be string format for shapefile\n",
    "samples_df[\"start\"] = pd.to_datetime(samples_df[\"start\"], dayfirst=True).dt.strftime(\"%Y-%m-%d\")\n",
    "samples_df[\"end\"] = pd.to_datetime(samples_df[\"end\"], dayfirst=True).dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fc8b2db-ffde-4490-af7b-c8b72f860aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to clean any remaining mismatched data\n",
    "crop_dictionary = {\n",
    "    \"bananas\": \"banana\",\n",
    "    \"groundnuts\": \"groundnut\",\n",
    "    \"macadamia nuts\": \"macadamia\",\n",
    "    \"macadamia nut\": \"macadamia\",\n",
    "    \"ochra vegetables\": \"ochra\",\n",
    "    \"okra\": \"ochra\",\n",
    "    \"soyabean\": \"soyabean\",\n",
    "    \"sweet potatoes\": \"sweet potato\",\n",
    "    \"water melon\": \"watermelon\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "925912fc-13e9-4330-bcf8-95dbebe12542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use with original ODK toolkit output\n",
    "# cleaned_geom_column = None\n",
    "# use with modified ODK toolkit output. The column listed below must contain data in lat,lon format (i.e. -14.4,28.0)\n",
    "cleaned_geom_column = \"Cleaned_Coordinates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8c2eac1-6ede-4c01-aed6-79546ad70842",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cleaned_geom_column is None:\n",
    "    # Return cleaned samples from ECAAS ODK format, containing mix of points and polygons\n",
    "    cleaned_samples_df = create_geometries_from_ODK(samples_df)\n",
    "\n",
    "    # Create two geodataframes, one with point geometry and one with polygon geometry\n",
    "    points_gdf = cleaned_samples_df.drop([\"field_boundary_polygon\"], axis=\"columns\").copy()\n",
    "    polygons_gdf = cleaned_samples_df.set_geometry(\"field_boundary_polygon\", drop=True).copy()\n",
    "    polygons_gdf = polygons_gdf.loc[\n",
    "        ~cleaned_samples_df[\"field_boundary_polygon\"].isna(), :\n",
    "    ]\n",
    "\n",
    "else:\n",
    "    # Retun geodataframe containing point geometry, extracted from cleaned_geom_column\n",
    "    points_gdf = create_geometries_from_manual_edit(samples_df, cleaned_geom_column)\n",
    "    polygons_gdf = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9043493-3fdc-462a-b030-aea0710dcdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with columns of interest and corresponding 10-character names\n",
    "col_rename_dict = {\n",
    "    \"start\": \"start\",\n",
    "    \"end\": \"end\",\n",
    "    \"field_fallow\": \"fallow\",\n",
    "    \"primary_crop_type\": \"pri_type\",\n",
    "    \"primary_crop\": \"pri_crop\",\n",
    "    \"crop_development\": \"crop_dev\",\n",
    "    \"multiple_crops\": \"multi_crop\",\n",
    "    \"multiple_crops_percentage\": \"multi_per\",\n",
    "    \"secondary_crop\": \"sec_crop\",\n",
    "    \"geometry\": \"geometry\",\n",
    "}\n",
    "    \n",
    "# Export polygons\n",
    "if polygons_gdf is not None:\n",
    "\n",
    "    polygons_gdf[col_rename_dict.keys()].to_file(\"data/cleaned_polygons.geojson\")\n",
    "    polygons_gdf[col_rename_dict.keys()].rename(columns=col_rename_dict).to_file(\n",
    "        \"data/cleaned_polygons.shp\"\n",
    "    )\n",
    "\n",
    "# Add additional column to specify the point location\n",
    "col_rename_dict[\"point_location\"] = \"point_loc\"\n",
    "\n",
    "# Export points\n",
    "points_gdf[col_rename_dict.keys()].to_file(\"data/cleaned_points.geojson\")\n",
    "points_gdf[col_rename_dict.keys()].rename(columns=col_rename_dict).to_file(\n",
    "    \"data/cleaned_points.shp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "633516d0-c553-43b6-bc15-7411356a3398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to cleaned data from previous step\n",
    "path = \"./data/cleaned_points.geojson\"\n",
    "# Load input data\n",
    "input_data = gpd.read_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7009fbb8-897b-427b-a1ea-4f463fabece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date fields to datetimes\n",
    "input_data[\"start\"] = pd.to_datetime(input_data[\"start\"], yearfirst=True)\n",
    "input_data[\"end\"] = pd.to_datetime(input_data[\"end\"], yearfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30706d0f-3bbc-4560-87a3-cb09b8bcee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows with multiple crops or fallow fields\n",
    "multiple_crop_condition = input_data.loc[:, \"multiple_crops\"] == \"yes\"\n",
    "fallow_field_condition = input_data.loc[:, \"field_fallow\"] == \"yes\"\n",
    "\n",
    "# Split datasets\n",
    "single_crops = input_data.loc[\n",
    "    (multiple_crop_condition == False) & (fallow_field_condition == False), :\n",
    "].copy()\n",
    "\n",
    "multiple_crops = input_data.loc[\n",
    "    (multiple_crop_condition == True) & (fallow_field_condition == False), :\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf115c82-f69b-4097-8172-b2a9ac2b39a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_crops_subset = single_crops[single_crops.groupby('primary_crop').primary_crop.transform('count')>=10].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "284c156b-43c3-4ee0-bf29-99bd73ce8d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Dictionary:\n",
      "{'beans': 0, 'cassava': 1, 'cotton': 2, 'groundnut': 3, 'maize': 4, 'millet': 5, 'sorghum': 6, 'soybean': 7, 'sunflower': 8, 'sweet potato': 9}\n"
     ]
    }
   ],
   "source": [
    "# Select field to label\n",
    "field = \"primary_crop\"\n",
    "\n",
    "# Fit label encoder to match classes to numeric labels\n",
    "le = LabelEncoder()\n",
    "le.fit(single_crops_subset[field])\n",
    "\n",
    "# Get a list of the crop types\n",
    "classes = list(le.classes_)\n",
    "\n",
    "# Assign numeric label for each class\n",
    "single_crops_subset[\"label\"] = le.transform(single_crops_subset[field])\n",
    "\n",
    "# Create a dictionary mapping classes to numeric labels\n",
    "class_dictionary = {crop_class: int(le.transform([crop_class])[0]) for crop_class in classes}\n",
    "print(\"Class Dictionary:\")\n",
    "print(class_dictionary)\n",
    "\n",
    "# Export class dictionary\n",
    "with open(\"data/class_labels.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(class_dictionary, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9479b81c-a226-4c93-b634-00478b5ebf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a flag to convert to polygons:\n",
    "use_polygons = True\n",
    "\n",
    "if use_polygons:\n",
    "    # Convert from lat,lon to EPSG:6933 (projection in metres)\n",
    "    single_crops_subset = single_crops_subset.to_crs(\"EPSG:6933\")\n",
    "\n",
    "    # Buffer geometry to get a square - only if trying to sample multiple pixels\n",
    "    buffer_radius_m = 15\n",
    "    single_crops_subset.geometry = single_crops_subset.geometry.buffer(buffer_radius_m, cap_style=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78be9602-6ed8-4529-88e4-56bee0800ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2022-04-11 00:00:00'), Timestamp('2022-04-19 00:00:00'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_crops_subset.start.min(), single_crops_subset.end.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95c0e8e8-2a04-4746-b365-5b1df4865198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2021-07-01 00:00:00')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_start_date = pd.Timestamp(\n",
    "    year=start_date.year, month=start_date.month, day=1\n",
    ")  - pd.DateOffset(months=9)\n",
    "query_start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30e2dfd9-c938-4e28-be72-9275c39e6a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2022-03-31 23:59:00')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_end_date = pd.Timestamp(\n",
    "    year=start_date.year, month=start_date.month, day=1\n",
    ") - pd.DateOffset(minutes=1)\n",
    "query_end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "916c1232-5e6b-494a-8ccd-c505d16871e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query start: 2021-07-01 00:00:00\n",
      "Query end: 2022-03-31 23:59:00\n"
     ]
    }
   ],
   "source": [
    "start_date = single_crops_subset.start.min()\n",
    "end_date = single_crops_subset.end.max()\n",
    "\n",
    "query_start_date = pd.Timestamp(\n",
    "    year=start_date.year, month=start_date.month, day=1\n",
    ") - pd.DateOffset(months=9)\n",
    "query_end_date = pd.Timestamp(\n",
    "    year=start_date.year, month=start_date.month, day=1\n",
    ") - pd.DateOffset(minutes=1)\n",
    "print(f\"Query start: {query_start_date}\")\n",
    "print(f\"Query end: {query_end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ffaf65-0ed6-4bca-aa6c-6e26e99d0ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a general query\n",
    "time = (query_start_date, query_end_date)\n",
    "resolution = (-10, 10)\n",
    "output_crs = \"EPSG:6933\"\n",
    "\n",
    "query = {\n",
    "    \"time\": time,\n",
    "    \"resolution\": resolution,\n",
    "    \"output_crs\": output_crs,\n",
    "}\n",
    "\n",
    "# Export query to pickle file for future re-use\n",
    "with open('results/query.pickle', 'wb') as f:\n",
    "    pickle.dump(query, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae150ce-21b6-4690-bca7-56c1ffbe47d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_function_over_custom_times(ds, func, func_name, time_ranges):\n",
    "    \"\"\"Apply generic function over an xarray dataset\"\"\"\n",
    "\n",
    "    output_list = []\n",
    "\n",
    "    for timelabel, timeslice in time_ranges.items():\n",
    "\n",
    "        if isinstance(timeslice, slice):\n",
    "            ds_timeslice = ds.sel(time=timeslice)\n",
    "        else:\n",
    "            ds_timeslice = ds.sel(time=timeslice, method=\"nearest\")\n",
    "\n",
    "        ds_modified = func(ds_timeslice)\n",
    "\n",
    "        rename_dict = {\n",
    "            key: f\"{key}_{func_name}_{timelabel}\" for key in list(ds_modified.keys())\n",
    "        }\n",
    "\n",
    "        ds_modified = ds_modified.rename(name_dict=rename_dict)\n",
    "\n",
    "        if \"time\" in list(ds_modified.coords):\n",
    "            ds_modified = ds_modified.reset_coords().drop_vars([\"time\", \"spatial_ref\"])\n",
    "\n",
    "        output_list.append(ds_modified)\n",
    "\n",
    "    return output_list\n",
    "\n",
    "\n",
    "# Define functions to load features\n",
    "def feature_layers(query):\n",
    "    \"\"\"Compute feature layers according to datacube query\"\"\"\n",
    "    \n",
    "#     query = static_query.copy() # include to make sure original query isn't modified\n",
    "\n",
    "    # Connnect to datacube\n",
    "    dc = datacube.Datacube(app=\"crop_type_ml\")\n",
    "\n",
    "#     # Check query for required time ranges and remove them\n",
    "#     if all(\n",
    "#         [\n",
    "#             key in query.keys()\n",
    "#             for key in [\n",
    "#                 \"time_ranges\",\n",
    "#                 \"annual_geomedian_times\",\n",
    "#                 \"semiannual_geomedian_times\",\n",
    "#             ]\n",
    "#         ]\n",
    "#     ):\n",
    "#         pass\n",
    "#     else:\n",
    "#         print(\n",
    "#             \"Query missing at least one of time_ranges, annual_geomedian_times, or semiannual_geomedian_times\"\n",
    "#         )\n",
    "#         sys.exit(1)\n",
    "\n",
    "#     # ----------------- STORE TIME RANGES FOR CUSTOM QUERIES -----------------\n",
    "#     # This removes these items from the query so it can be used for loads\n",
    "#     time_ranges = query.pop(\"time_ranges\")\n",
    "#     annual_geomedian_times = query.pop(\"annual_geomedian_times\")\n",
    "#     semiannual_geomedian_times = query.pop(\"semiannual_geomedian_times\")\n",
    "\n",
    "    # ----------------- HARDCODE TIME RANGES FOR CUSTOM QUERIES -----------------\n",
    "    # This means the function can be used without modifying the datacube query\n",
    "    \n",
    "    time_ranges = {\n",
    "        \"Q3_2021\": slice(\"2021-08-01\", \"2021-10-31\"),\n",
    "        \"Q4_2021\": slice(\"2021-11-01\", \"2022-01-31\"),\n",
    "        \"Q1_2022\": slice(\"2022-02-01\", \"2022-04-30\"),\n",
    "    }\n",
    "    \n",
    "    # !!! FOR ZAMBIA, S1 DATA IS MISSING FOR HALF THE COUNTRY IN 2022 !!!\n",
    "    s1_time_ranges = {\n",
    "        \"Q3_2021\": slice(\"2021-08-01\", \"2022-10-31\"),\n",
    "        \"Q4_2021\": slice(\"2021-11-01\", \"2022-01-31\"),\n",
    "    }\n",
    "    \n",
    "    annual_geomedian_times = {\n",
    "        \"annual_2021\": \"2021-01-01\",\n",
    "    }\n",
    "    semiannual_geomedian_times = {\n",
    "        \"semiannual_2021_01\": \"2021-01-01\",\n",
    "        \"semiannual_2021_06\": \"2021-06-01\",\n",
    "    }\n",
    "\n",
    "    # ----------------- DEFINE MEASUREMENTS TO USE FOR EACH PRODUCT -----------------\n",
    "\n",
    "    s2_measurements = [\n",
    "        \"blue\",\n",
    "        \"green\",\n",
    "        \"red\",\n",
    "        \"nir\",\n",
    "        \"swir_1\",\n",
    "        \"swir_2\",\n",
    "        \"red_edge_1\",\n",
    "        \"red_edge_2\",\n",
    "        \"red_edge_3\",\n",
    "    ]\n",
    "\n",
    "    s2_geomad_measurements = s2_measurements + [\"smad\", \"emad\", \"bcmad\"]\n",
    "\n",
    "    s1_measurements = [\"vv\", \"vh\"]\n",
    "\n",
    "    fc_measurements = [\"bs\", \"pv\", \"npv\", \"ue\"]\n",
    "\n",
    "    rainfall_measurements = [\"rainfall\"]\n",
    "\n",
    "    slope_measurements = [\"slope\"]\n",
    "\n",
    "    # ----------------- S2 CUSTOM GEOMEDIANS -----------------\n",
    "    # These are designed to take the geomedian for every range in time_ranges\n",
    "    # This is controlled through the input query\n",
    "\n",
    "    ds = load_ard(\n",
    "        dc=dc,\n",
    "        products=[\"s2_l2a\"],\n",
    "        measurements=s2_measurements,\n",
    "        group_by=\"solar_day\",\n",
    "        verbose=False,\n",
    "        **query,\n",
    "    )\n",
    "\n",
    "    # Apply geomedian over time ranges and calculate band indices\n",
    "    s2_geomad_list = apply_function_over_custom_times(\n",
    "        ds, geomedian_with_indices_wrapper, \"s2\", time_ranges\n",
    "    )\n",
    "\n",
    "    # ----------------- S2 ANNUAL GEOMEDIAN -----------------\n",
    "\n",
    "    # Update query to use annual_geomedian_times\n",
    "    ds_annual_geomad_query = query.copy()\n",
    "    query_times = list(annual_geomedian_times.values())\n",
    "    ds_annual_geomad_query.update({\"time\": (query_times[0], query_times[-1])})\n",
    "\n",
    "    # load s2 annual geomedian\n",
    "    ds_s2_geomad = dc.load(\n",
    "        product=\"gm_s2_annual\",\n",
    "        measurements=s2_geomad_measurements,\n",
    "        **ds_annual_geomad_query,\n",
    "    )\n",
    "\n",
    "    # Calculate band indices\n",
    "    s2_annual_list = apply_function_over_custom_times(\n",
    "        ds_s2_geomad, indices_wrapper, \"s2\", annual_geomedian_times\n",
    "    )\n",
    "\n",
    "    # ----------------- S2 SEMIANNUAL GEOMEDIAN -----------------\n",
    "\n",
    "    # Update query to use semiannual_geomedian_times\n",
    "    ds_semiannual_geomad_query = query.copy()\n",
    "    query_times = list(semiannual_geomedian_times.values())\n",
    "    ds_semiannual_geomad_query.update({\"time\": (query_times[0], query_times[-1])})\n",
    "\n",
    "    # load s2 semiannual geomedian\n",
    "    ds_s2_semiannual_geomad = dc.load(\n",
    "        product=\"gm_s2_semiannual\",\n",
    "        measurements=s2_geomad_measurements,\n",
    "        **ds_semiannual_geomad_query,\n",
    "    )\n",
    "\n",
    "    # Calculate band indices\n",
    "    s2_semiannual_list = apply_function_over_custom_times(\n",
    "        ds_s2_semiannual_geomad, indices_wrapper, \"s2\", semiannual_geomedian_times\n",
    "    )\n",
    "\n",
    "    # ----------------- S1 CUSTOM GEOMEDIANS -----------------\n",
    "\n",
    "    # Update query to suit Sentinel 1\n",
    "    s1_query = query.copy()\n",
    "    s1_query.update({\"sat_orbit_state\": \"ascending\"})\n",
    "\n",
    "    # Load s1\n",
    "    s1_ds = load_ard(\n",
    "        dc=dc,\n",
    "        products=[\"s1_rtc\"],\n",
    "        measurements=s1_measurements,\n",
    "        group_by=\"solar_day\",\n",
    "        verbose=False,\n",
    "        **s1_query,\n",
    "    )\n",
    "\n",
    "    # Apply geomedian\n",
    "    s1_geomad_list = apply_function_over_custom_times(\n",
    "        s1_ds, xr_geomedian, \"s1_xrgm\", s1_time_ranges\n",
    "    )\n",
    "\n",
    "    # -------- LANDSAT BIMONTHLY FRACTIONAL COVER -----------\n",
    "\n",
    "    # Update query to suit fractional cover\n",
    "    fc_query = query.copy()\n",
    "    fc_query.update({\"resampling\": \"bilinear\", \"measurements\": fc_measurements})\n",
    "\n",
    "    # load fractional cover\n",
    "    ds_fc = dc.load(product=\"fc_ls\", collection_category=\"T1\", **fc_query)\n",
    "\n",
    "    # Apply median\n",
    "    fc_median_list = apply_function_over_custom_times(\n",
    "        ds_fc, median_wrapper, \"median\", time_ranges\n",
    "    )\n",
    "\n",
    "    # -------- CHIRPS MONTHLY RAINFALL -----------\n",
    "\n",
    "    # Update query to suit CHIRPS rainfall\n",
    "    rainfall_query = query.copy()\n",
    "    rainfall_query.update(\n",
    "        {\"resampling\": \"bilinear\", \"measurements\": rainfall_measurements}\n",
    "    )\n",
    "\n",
    "    # Load rainfall and update no data values\n",
    "    ds_rainfall = dc.load(product=\"rainfall_chirps_monthly\", **rainfall_query)\n",
    "\n",
    "    rainfall_nodata = -9999.0\n",
    "    ds_rainfall = ds_rainfall.where(\n",
    "        ds_rainfall.rainfall != rainfall_nodata, other=np.nan\n",
    "    )\n",
    "\n",
    "    # Apply mean\n",
    "    rainfall_mean_list = apply_function_over_custom_times(\n",
    "        ds_rainfall, mean_wrapper, \"mean\", time_ranges\n",
    "    )\n",
    "\n",
    "    # -------- DEM SLOPE -----------\n",
    "    slope_query = query.copy()\n",
    "    slope_query.update(\n",
    "        {\n",
    "            \"resampling\": \"bilinear\",\n",
    "            \"measurements\": slope_measurements,\n",
    "            \"time\": \"2000-01-01\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Load slope data and update no data values and coordinates\n",
    "    ds_slope = dc.load(product=\"dem_srtm_deriv\", **slope_query)\n",
    "\n",
    "    slope_nodata = -9999.0\n",
    "    ds_slope = ds_slope.where(ds_slope != slope_nodata, np.nan)\n",
    "\n",
    "    ds_slope = ds_slope.squeeze(\"time\")#.reset_coords(\"time\", drop=True)\n",
    "\n",
    "    # ----------------- FINAL MERGED XARRAY -----------------\n",
    "\n",
    "    # Create a list to keep all items for final merge\n",
    "    ds_list = []\n",
    "    ds_list.extend(s2_geomad_list)\n",
    "    ds_list.extend(s2_annual_list)\n",
    "    ds_list.extend(s2_semiannual_list)\n",
    "    ds_list.extend(s1_geomad_list)\n",
    "    ds_list.extend(fc_median_list)\n",
    "    ds_list.extend(rainfall_mean_list)\n",
    "    ds_list.append(ds_slope)\n",
    "\n",
    "    ds_final = xr.merge(ds_list)\n",
    "\n",
    "    return ds_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
